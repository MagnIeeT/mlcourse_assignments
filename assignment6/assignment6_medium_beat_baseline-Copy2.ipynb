{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating baselines in \"How good is your Medium article?\"\n",
    "    \n",
    "<img src='../../img/medium_claps.jpg' width=40% />\n",
    "\n",
    "\n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"A6 baseline\" (~1.45 Public LB score). Do not forget about our shared [\"primitive\" baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline) - you'll find something valuable there.\n",
    "\n",
    "**Your task:**\n",
    " 1. \"Freeride\". Come up with good features to beat the baseline \"A6 baseline\" (for now, public LB is only considered)\n",
    " 2. You need to name your [team](https://www.kaggle.com/c/how-good-is-your-medium-article/team) (out of 1 person) in full accordance with the [course rating](https://drive.google.com/open?id=19AGEhUQUol6_kNLKSzBsjcGUU3qWy3BNUg8x8IFkO3Q). You can think of it as a part of the assignment. 16 credits for beating the mentioned baseline and correct team naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack,coo_matrix\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features_and_write(path_to_data,\n",
    "#                                inp_filename, is_train=True):  \n",
    "            \n",
    "#     content_list = []\n",
    "#     published_list =[]\n",
    "#     title_list =[]\n",
    "#     author_list =[]\n",
    "#     domain_list =[]\n",
    "#     tags_list = []\n",
    "    \n",
    "\n",
    "#     with open(os.path.join(path_to_data, inp_filename), \n",
    "#               encoding='utf-8') as inp_json_file:\n",
    "        \n",
    "#         for line in tqdm_notebook(inp_json_file):\n",
    "#             json_data = read_json_line(line)\n",
    "            \n",
    "            \n",
    "#             content = json_data['content'].replace('\\n',' ').replace('\\r',' ')\n",
    "#             content_list.append(strip_tags(content))\n",
    "            \n",
    "#             published_list.append(json_data['published']['$date'])\n",
    "            \n",
    "#             title_list.append(strip_tags(json_data['title']).split('\\u2013')[0].strip().replace('\\n',' ').replace('\\r',' '))\n",
    "            \n",
    "#             author_list.append(json_data['meta_tags']['author'].strip())\n",
    "            \n",
    "#             domain_list.append(json_data['domain'])\n",
    "            \n",
    "#             tags_str = []\n",
    "#             soup = BeautifulSoup(content, 'lxml')\n",
    "#             try:\n",
    "#                 tag_block = soup.find('ul', class_='tags')\n",
    "#                 tags = tag_block.find_all('a')\n",
    "#                 for tag in tags:\n",
    "#                     tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "#                 tags = ' '.join(tags_str)\n",
    "#             except Exception:\n",
    "#                 tags = 'None'\n",
    "            \n",
    "#             tags_list.append(tags)\n",
    "            \n",
    "#         df = pd.DataFrame()\n",
    "#         df['content'] = content_list\n",
    "#         df['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "#         df['title'] = title_list\n",
    "#         df['author'] = author_list\n",
    "#         df['domain'] = domain_list\n",
    "#         df['tags'] = tags_list\n",
    "        \n",
    "#         if is_train:\n",
    "#             df.sort_values(by ='published',inplace=True)\n",
    "\n",
    "#         features = ['content', 'published', 'title', 'author', 'domain', 'tags']\n",
    "    \n",
    "#         prefix = 'train' if is_train else 'test'\n",
    "        \n",
    "#         for feat in features:\n",
    "#             df[feat].to_csv(os.path.join(path_to_data,'{}_{}.txt'.format(prefix, feat)),sep=' ',index=None,header =None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):\n",
    "    \n",
    "    features = ['content', 'published', 'title', 'author', 'domain', 'tags']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data,\n",
    "                                       '{}_{}.txt'.format(prefix, feat)),\n",
    "                          'w', encoding='utf-8')\n",
    "                     for feat in features]\n",
    "    \n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "        \n",
    "\n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            json_data = read_json_line(line)\n",
    "            \n",
    "            \n",
    "            content = json_data['content'].replace('\\n',' ').replace('\\r',' ')\n",
    "            feature_files[0].write(strip_tags(content)+'\\n')\n",
    "            \n",
    "            feature_files[1].write(json_data['published']['$date']+'\\n')\n",
    "            \n",
    "            feature_files[2].write(strip_tags(json_data['title']).split('\\u2013')[0].strip().replace('\\n',' ').replace('\\r',' ')+'\\n')\n",
    "            \n",
    "            feature_files[3].write(json_data['meta_tags']['author'].strip()+'\\n')\n",
    "            \n",
    "            feature_files[4].write(json_data['domain']+'\\n')\n",
    "            \n",
    "            tags_str = []\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            try:\n",
    "                tag_block = soup.find('ul', class_='tags')\n",
    "                tags = tag_block.find_all('a')\n",
    "                for tag in tags:\n",
    "                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "                tags = ' '.join(tags_str)\n",
    "            except Exception:\n",
    "                tags = 'None'\n",
    "            \n",
    "            feature_files[5].write(tags+'\\n')\n",
    "\n",
    "        for feature_file in feature_files:\n",
    "            feature_file.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data/Medium/traintestunsorted' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561380c2c5754c7cbd49bddbbcb8b484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09864886c69e4869b0f2df2f9d8c8443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_content = TfidfVectorizer(os.path.join(PATH_TO_DATA,'train_content.txt'),ngram_range=(1,2),max_features=100000)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_content.txt'),encoding=\"utf8\") as train_content:\n",
    "    X_train_content_sparse = tfidf_content.fit_transform(train_content)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_content.txt'),encoding=\"utf8\") as test_content:\n",
    "    X_test_content_sparse = tfidf_content.fit_transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_content_sparse.shape,X_test_content_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving for quick access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# sparse.save_npz(\"./data/medium/X_test_content_sparse_total_bigram2.npz\", X_test_content_sparse)\n",
    "X_test_content_sparse = sparse.load_npz(\"./data/medium/X_test_content_sparse_total_bigram2.npz\")\n",
    "\n",
    "# sparse.save_npz(\"./data/medium/X_train_content_sparse_total_bigram2.npz\", X_train_content_sparse)\n",
    "X_train_content_sparse = sparse.load_npz(\"./data/medium/X_train_content_sparse_total_bigram2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Titles **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_title = TfidfVectorizer(os.path.join(PATH_TO_DATA,'train_title.txt'),ngram_range=(1,2),max_features=100000)\n",
    "#                               strip_accents='unicode', stop_words='english')\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_title.txt'),encoding=\"utf8\") as train_title:\n",
    "    X_train_title_sparse = tfidf_title.fit_transform(train_title)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA,'test_title.txt'),encoding=\"utf8\") as test_title:\n",
    "    X_test_title_sparse = tfidf_title.fit_transform(test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (34645, 100000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title_sparse.shape,X_test_title_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** tags **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tags = TfidfVectorizer(os.path.join(PATH_TO_DATA,'total_tags.txt'),ngram_range=(1,2),max_features=100000,stop_words='english')\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_tags.txt'),encoding=\"utf8\") as train_tags:\n",
    "    X_train_tags_sparse = tfidf_tags.fit_transform(train_tags)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA,'test_tags.txt'),encoding=\"utf8\") as test_tags:\n",
    "    X_test_tags_sparse = tfidf_tags.fit_transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (34645, 100000))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tags_sparse.shape,X_test_tags_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Authors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 5843), (34645, 5843))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_author_ = pd.read_csv(os.path.join(PATH_TO_DATA,'train_author.txt'),header=None,usecols=[0],names=[\"authors\"])\n",
    "test_author_ = pd.read_csv(os.path.join(PATH_TO_DATA,'test_author.txt'),header=None,usecols=[0],names=[\"authors\"])\n",
    "\n",
    "authors = pd.concat([train_author_,test_author_],axis=0,sort=False)\n",
    "\n",
    "# train_author_list = set(train_author_.authors.value_counts().head(500).index)\n",
    "train_author_list = set(train_author_.authors.value_counts().index)\n",
    "test_author_list = set(test_author_.authors.value_counts().index)\n",
    "\n",
    "common_author = train_author_list.intersection(test_author_list)\n",
    "\n",
    "def encodecommonauthors(x):\n",
    "    if x in common_author: return x\n",
    "    else: return 'unknown'\n",
    "    \n",
    "authors = authors.authors.map(encodecommonauthors)\n",
    "\n",
    "authors = pd.get_dummies(authors,drop_first=True)\n",
    "\n",
    "X_train_author_sparse = authors.iloc[:train_author_.shape[0],:].values\n",
    "X_test_author_sparse  = authors.iloc[train_author_.shape[0]:,:].values\n",
    "\n",
    "X_train_author_sparse.shape, X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 43880), (34645, 43880))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.concat([train_author_,test_author_],axis=0,sort=False)\n",
    "authors = pd.get_dummies(authors)\n",
    "\n",
    "X_train_author_sparse = authors.iloc[:train_author_.shape[0],]\n",
    "X_test_author_sparse = authors.iloc[train_author_.shape[0]:,]\n",
    "\n",
    "X_train_author_sparse.shape, X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 31331), (34645, 31331))"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LE = LabelEncoder()\n",
    "# LE.fit(authors.values.reshape(1,-1)[0])\n",
    "# X_train_author_LE = LE.transform(train_author_.values.reshape(1,-1)[0])\n",
    "# X_test_author_LE  = LE.transform(test_author_.values.reshape(1,-1)[0])\n",
    "\n",
    "# OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "# OHE.fit(X_train_author_LE.reshape(-1,1))\n",
    "\n",
    "# X_train_author_sparse = OHE.transform(X_train_author_LE.reshape(-1,1))\n",
    "# X_test_author_sparse  = OHE.transform(X_test_author_LE.reshape(-1,1))\n",
    "\n",
    "# X_train_author_sparse.shape, X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** domain **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 2), (34645, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "train_domain_ = pd.read_csv(os.path.join(PATH_TO_DATA,'train_domain.txt'),header=None,usecols=[0],names=['domains'])\n",
    "test_domain_ = pd.read_csv(os.path.join(PATH_TO_DATA,'test_domain.txt'),header=None,usecols=[0],names = ['domains'])\n",
    "\n",
    "domains = pd.concat([train_domain_,test_domain_],axis=0,sort=False)\n",
    "\n",
    "def toptwodomain(x):\n",
    "    if x=='medium.com': return 0\n",
    "    elif x=='hackernoon.com': return 1\n",
    "    else: return 2\n",
    "domains = domains.domains.map(toptwodomain)\n",
    "\n",
    "domains = pd.get_dummies(domains,drop_first=True)\n",
    "\n",
    "X_train_domain_sparse = domains.iloc[:train_domain_.shape[0],:].values\n",
    "X_test_domain_sparse  = domains.iloc[train_domain_.shape[0]:,:].values\n",
    "\n",
    "X_train_domain_sparse.shape, X_test_domain_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Date Columns **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time = pd.read_csv(os.path.join(PATH_TO_DATA,'train_published.txt'),header=None,names=['date'],\n",
    "                                            parse_dates =['date'])\n",
    "X_test_time = pd.read_csv(os.path.join(PATH_TO_DATA,'test_published.txt'),header=None,names=['date'],\n",
    "                                         parse_dates =['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 1), (34645, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time.shape,X_test_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    new_df = pd.DataFrame(index=df.index)\n",
    "    hour = df['date'].apply(lambda ts: ts.hour)\n",
    "    new_df['morning'] = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    new_df[\"day\"] = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    new_df[\"evening\"] = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    new_df[\"night\"] = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    new_df['is_weekend'] = df['date'].dt.weekday.isin([6,7]).astype('int')\n",
    "#     new_df['year'] = df['date'].dt.year\n",
    "#     new_df['month'] = df['date'].dt.month\n",
    "#     new_df['weekday'] = df['date'].dt.weekday\n",
    "#     new_df['hour'] = df['date'].dt.hour    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 5), (34645, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time_features_sparse = add_time_features(X_train_time)\n",
    "X_test_time_features_sparse = add_time_features(X_test_time)\n",
    "X_train_time_features_sparse.shape,X_test_time_features_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** length features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA,'train_content.txt'),encoding=\"utf8\") as train_content:\n",
    "    length =[] \n",
    "    for line in train_content:\n",
    "        length.append(len(line))\n",
    "X_train_time_features_sparse['length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_content.txt'),encoding=\"utf8\") as test_content:\n",
    "    length =[] \n",
    "    for line in test_content:\n",
    "        length.append(len(line))\n",
    "X_test_time_features_sparse['length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_tags.txt'),encoding=\"utf8\") as train_tags:\n",
    "    length =[] \n",
    "    for line in train_tags:\n",
    "        length.append(len(line))\n",
    "X_train_time_features_sparse['tag_length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_tags.txt'),encoding=\"utf8\") as test_tags:\n",
    "    length =[] \n",
    "    for line in test_tags:\n",
    "        length.append(len(line))\n",
    "X_test_time_features_sparse['tag_length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train_time_features_sparse)\n",
    "X_train_time_features_sparse[X_train_time_features_sparse.columns] = minmax.transform(X_train_time_features_sparse)\n",
    "X_test_time_features_sparse[X_test_time_features_sparse.columns] = minmax.transform(X_test_time_features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(X_test_time_features_sparse.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morning</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>length</th>\n",
       "      <th>tag_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.289720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010202</td>\n",
       "      <td>0.140187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.336449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>0.439252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   morning  day  evening  night  is_weekend    length  tag_length\n",
       "0      0.0  0.0      1.0    0.0         0.0  0.015903    0.289720\n",
       "1      1.0  0.0      0.0    0.0         0.0  0.024894    0.000000\n",
       "2      0.0  1.0      0.0    0.0         1.0  0.010202    0.140187\n",
       "3      1.0  0.0      0.0    0.0         0.0  0.009842    0.336449\n",
       "4      0.0  1.0      0.0    0.0         1.0  0.012265    0.439252"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time_features_sparse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>log_recomments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24068</th>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>1.09861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59081</th>\n",
       "      <td>1970-01-01 00:00:00.001</td>\n",
       "      <td>2.56495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54757</th>\n",
       "      <td>1970-01-18 03:21:32.400</td>\n",
       "      <td>0.69315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32182</th>\n",
       "      <td>1987-12-08 21:45:00.000</td>\n",
       "      <td>1.09861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33152</th>\n",
       "      <td>2003-12-29 17:00:00.000</td>\n",
       "      <td>3.52636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date  log_recomments\n",
       "24068 1970-01-01 00:00:00.001         1.09861\n",
       "59081 1970-01-01 00:00:00.001         2.56495\n",
       "54757 1970-01-18 03:21:32.400         0.69315\n",
       "32182 1987-12-08 21:45:00.000         1.09861\n",
       "33152 2003-12-29 17:00:00.000         3.52636"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_date = X_train_time.copy()\n",
    "y_train_date['log_recomments'] = y_train\n",
    "y_train_date.sort_values(by = 'date',inplace=True)\n",
    "y_train_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse\n",
    "# X_train_content_sparse = sparse.load_npz(\"./data/medium/X_train_content_sparse_total_bigram.npz\")\n",
    "# X_test_content_sparse = sparse.load_npz(\"./data/medium/X_test_content_sparse_total_bigram.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 243889), (34645, 243889))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse = hstack([\n",
    "                          X_train_domain_sparse,\n",
    "                          X_train_content_sparse,\n",
    "#                           X_train_title_sparse,                          \n",
    "                          X_train_tags_sparse,\n",
    "                          X_train_author_sparse,\n",
    "                          X_train_time_features_sparse]).tocsr()\n",
    "\n",
    "X_test_sparse = hstack([\n",
    "                         X_test_domain_sparse,\n",
    "                         X_test_content_sparse,\n",
    "#                          X_test_title_sparse,\n",
    "                         X_test_tags_sparse,\n",
    "                         X_test_author_sparse,\n",
    "                         X_test_time_features_sparse]).tocsr()\n",
    "\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1019100602857763"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse)) ## added domain no titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.154999348805752"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse)) ## added domain no titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0743296119640227"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse)) ## added domain no titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.33328, 2.5208363800085385)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge.fit(X_train_sparse,y_train)\n",
    "prediction = lridge.predict(X_test_sparse)\n",
    "\n",
    "pred_adjusted = prediction+ (4.33328 -prediction.mean())\n",
    "pred_adjusted.mean(),prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/medium_submission_3.csv')\n",
    "#CV 1.1019100602857763 #LB 1.668262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/medium_submission_4.csv')\n",
    "#CV 1.0743296119640227 #LB 1.65072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70615878, -1.67787463, -1.60717713, -1.52979893, -1.49164347]),\n",
       " -1.602530587673937)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.73459287, -1.61540689, -1.49328139]), -1.6144270493039015)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=3,n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.6961397 , -1.66503504, -1.57940485, -1.51182441, -1.47267521]),\n",
       " -1.5850158420836533)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70678507, -1.66486316, -1.574324  , -1.51921109, -1.47291004]),\n",
       " -1.5876186715697718)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month,feature + title(5 ngram) + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70326353, -1.66437512, -1.5739535 , -1.51747089, -1.47336411]),\n",
       " -1.5864854295562016)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=500)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() \n",
    "# only author common, time +content(train corpus) + removed hour,year, weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.79021982, -1.76944994, -1.67921221, -1.64240117, -1.58974062]),\n",
       " -1.6942047504117308)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +content(train corpus)+title(train corpus) + removed hour,weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75029904, -1.71864653, -1.63777537, -1.58549061, -1.53951762]),\n",
       " -1.6463458332448966)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +content(train corpus) + removed hour,weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.80411863, -1.76962978, -1.66307081, -1.60140272, -1.54928635]),\n",
       " -1.6775016574455894)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +title + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71383703, -1.67039702, -1.58319636, -1.52311857, -1.47406608]),\n",
       " -1.5929230112113395)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75029904, -1.71864653, -1.63777537, -1.58549061, -1.53951762]),\n",
       " -1.6463458332448966)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time+ content(bigram, 50000) + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.7511022 , -1.71865133, -1.63760093, -1.58540646, -1.54022222]),\n",
       " -1.6465966285312508)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ content(bigram, 50000) + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75552844, -1.72424925, -1.65143391, -1.59018058, -1.54537297]),\n",
       " -1.653353029870934)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ content(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.80465401, -1.76968117, -1.663078  , -1.60150013, -1.54924794]),\n",
       " -1.677632249227821)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ title(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71605728, -1.67320132, -1.58660663, -1.52797207, -1.47772642]),\n",
       " -1.5963127440676463)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(1000), domain, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71481073, -1.67048868, -1.58324289, -1.52315781, -1.47411238]),\n",
       " -1.5931624978304328)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77988321, -1.75231114, -1.68516628, -1.63273228, -1.58802091]),\n",
       " -1.6876227639716952)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78005177, -1.75267851, -1.68626226, -1.6318807 , -1.58865579]),\n",
       " -1.6879058038621024)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78776372, -1.7541245 , -1.69207366, -1.62452704, -1.59107438]),\n",
       " -1.6899126623792564)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78788731, -1.75173271, -1.67609661, -1.60911423, -1.57100427]),\n",
       " -1.6791670269193824)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + tags - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.79597596, -1.76370831, -1.68977071, -1.62458945, -1.58708861]),\n",
       " -1.6922266082441344)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + small tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77895346, -1.74125258, -1.66932919, -1.60010456, -1.55931242]),\n",
       " -1.669790442385595)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77880984, -1.74066763, -1.66815147, -1.6010553 , -1.5594876 ]),\n",
       " -1.669634365394295)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + reduced domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78088846, -1.75069073, -1.65618224, -1.59819464, -1.55226787]),\n",
       " -1.6676447868344884)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus +reduced authorlist+ reduced domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78248074, -1.75609869, -1.68068572, -1.62029946, -1.59025814]),\n",
       " -1.685964551629732)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus +reduced authorlist+ reduced domain + \n",
    "#small ngram tags and titles(nostopword) - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78240041, -1.75600557, -1.68051004, -1.6203664 , -1.58921474]),\n",
       " -1.685699431471828)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#small ngram tags and titles(nostopword) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75596149, -1.72707715, -1.65358406, -1.59496739, -1.54873665]),\n",
       " -1.6560653469113027)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "# - no tags -no title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.7554402 , -1.72698805, -1.66269248, -1.60270939, -1.5585561 ]),\n",
       " -1.661277245275749)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#  - no tags -no title + more authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75552844, -1.72424925, -1.65143391, -1.59018058, -1.54537297]),\n",
       " -1.653353029870934)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#  - no tags -no title + less authors(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.74941744, -1.71131357, -1.63078541, -1.55365962, -1.51169151]),\n",
       " -1.6313735095426722)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # no content +reduced authorlist+ reduced domain + \n",
    "#  -no title + less authors(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. Much more credits will be given to the winners in this competition, check [course roadmap](https://mlcourse.ai/roadmap). Do not spoil the assignment and the competition - don't share high-performing kernels (with MAE < 1.5).\n",
    "\n",
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "<img src='../../img/kaggle_shakeup.png' width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

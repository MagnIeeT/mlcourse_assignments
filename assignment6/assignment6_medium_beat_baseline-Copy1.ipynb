{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating baselines in \"How good is your Medium article?\"\n",
    "    \n",
    "<img src='../../img/medium_claps.jpg' width=40% />\n",
    "\n",
    "\n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"A6 baseline\" (~1.45 Public LB score). Do not forget about our shared [\"primitive\" baseline](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline) - you'll find something valuable there.\n",
    "\n",
    "**Your task:**\n",
    " 1. \"Freeride\". Come up with good features to beat the baseline \"A6 baseline\" (for now, public LB is only considered)\n",
    " 2. You need to name your [team](https://www.kaggle.com/c/how-good-is-your-medium-article/team) (out of 1 person) in full accordance with the [course rating](https://drive.google.com/open?id=19AGEhUQUol6_kNLKSzBsjcGUU3qWy3BNUg8x8IFkO3Q). You can think of it as a part of the assignment. 16 credits for beating the mentioned baseline and correct team naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack,coo_matrix\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):  \n",
    "            \n",
    "    content_list = []\n",
    "    published_list =[]\n",
    "    title_list =[]\n",
    "    author_list =[]\n",
    "    domain_list =[]\n",
    "    tags_list = []\n",
    "    \n",
    "\n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "        \n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            json_data = read_json_line(line)\n",
    "            \n",
    "            \n",
    "            content = json_data['content'].replace('\\n',' ').replace('\\r',' ')\n",
    "            content_list.append(strip_tags(content))\n",
    "            \n",
    "            published_list.append(json_data['published']['$date'])\n",
    "            \n",
    "            title_list.append(strip_tags(json_data['title']).split('\\u2013')[0].strip().replace('\\n',' ').replace('\\r',' '))\n",
    "            \n",
    "            author_list.append(json_data['meta_tags']['author'].strip())\n",
    "            \n",
    "            domain_list.append(json_data['domain'])\n",
    "            \n",
    "            tags_str = []\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            try:\n",
    "                tag_block = soup.find('ul', class_='tags')\n",
    "                tags = tag_block.find_all('a')\n",
    "                for tag in tags:\n",
    "                    tags_str.append(tag.text.translate({ord(' '):None, ord('-'):None}))\n",
    "                tags = ' '.join(tags_str)\n",
    "            except Exception:\n",
    "                tags = 'None'\n",
    "            \n",
    "            tags_list.append(tags)\n",
    "            \n",
    "        df = pd.DataFrame()\n",
    "        df['content'] = content_list\n",
    "        df['published'] = pd.to_datetime(published_list, format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        df['title'] = title_list\n",
    "        df['author'] = author_list\n",
    "        df['domain'] = domain_list\n",
    "        df['tags'] = tags_list\n",
    "        \n",
    "        if is_train:\n",
    "            df.sort_values(by ='published',inplace=True)\n",
    "\n",
    "        features = ['content', 'published', 'title', 'author', 'domain', 'tags']\n",
    "    \n",
    "        prefix = 'train' if is_train else 'test'\n",
    "        \n",
    "        for feat in features:\n",
    "            df[feat].to_csv(os.path.join(path_to_data,'{}_{}.txt'.format(prefix, feat)),sep=' ',index=None,header =None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data/Medium' # modify this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3b00200359478b8cb700e5462a08ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !type \"./data/medium/train_title.txt\" >> \"./data/medium/total_content.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_content = TfidfVectorizer(os.path.join(PATH_TO_DATA,'train_content.txt'),ngram_range=(1,2),max_features=100000)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_content.txt'),encoding=\"utf8\") as train_content:\n",
    "    X_train_content_sparse = tfidf_content.fit_transform(train_content)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_content.txt'),encoding=\"utf8\") as test_content:\n",
    "    X_test_content_sparse = tfidf_content.fit_transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (34645, 100000))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_content_sparse.shape,X_test_content_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving for quick access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"./data/medium/X_test_content_sparse_total_bigram.npz\", X_test_content_sparse)\n",
    "# X_test_content_sparse = sparse.load_npz(\"./data/medium/X_test_content_sparse_total.npz\")\n",
    "\n",
    "sparse.save_npz(\"./data/medium/X_train_content_sparse_total_bigram.npz\", X_train_content_sparse)\n",
    "# X_train_content_sparse = sparse.load_npz(\"./data/medium/X_train_content_sparse_total.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Titles **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_title = TfidfVectorizer(os.path.join(PATH_TO_DATA,'train_title_strip.txt'),ngram_range=(1,2),max_features=100000)\n",
    "#                               strip_accents='unicode', stop_words='english')\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_title_strip.txt'),encoding=\"utf8\") as train_title:\n",
    "    X_train_title_sparse = tfidf_title.fit_transform(train_title)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA,'test_title.txt'),encoding=\"utf8\") as test_title:\n",
    "    X_test_title_sparse = tfidf_title.fit_transform(test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (34645, 100000))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_title_sparse.shape,X_test_title_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** tags **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tags = TfidfVectorizer(os.path.join(PATH_TO_DATA,'total_tags.txt'),ngram_range=(1,2),max_features=100000,stop_words='english')\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_tags.txt'),encoding=\"utf8\") as train_tags:\n",
    "    X_train_tags_sparse = tfidf_tags.fit_transform(train_tags)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA,'test_tags.txt'),encoding=\"utf8\") as test_tags:\n",
    "    X_test_tags_sparse = tfidf_tags.fit_transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (34645, 100000))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tags_sparse.shape,X_test_tags_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Authors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 5841), (34645, 5841))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_author_ = pd.read_csv(os.path.join(PATH_TO_DATA,'train_author.txt'),header=None,usecols=[0],names=[\"authors\"])\n",
    "test_author_ = pd.read_csv(os.path.join(PATH_TO_DATA,'test_author.txt'),header=None,usecols=[0],names=[\"authors\"])\n",
    "\n",
    "authors = pd.concat([train_author_,test_author_],axis=0,sort=False)\n",
    "\n",
    "train_author_list = set(train_author_.authors.value_counts().head(500).index)\n",
    "# train_author_list = set(train_author_.authors.value_counts().index)\n",
    "test_author_list = set(test_author_.authors.value_counts().index)\n",
    "\n",
    "common_author = train_author_list.intersection(test_author_list)\n",
    "\n",
    "def encodecommonauthors(x):\n",
    "    if x in common_author: return x\n",
    "    else: return 'unknown'\n",
    "    \n",
    "authors = authors.authors.map(encodecommonauthors)\n",
    "\n",
    "authors = pd.get_dummies(authors,drop_first=True)\n",
    "\n",
    "X_train_author_sparse = authors.iloc[:train_author_.shape[0],:].values\n",
    "X_test_author_sparse  = authors.iloc[train_author_.shape[0]:,:].values\n",
    "\n",
    "X_train_author_sparse.shape, X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arman Anaturk        8\n",
       "Doc Ayomide          8\n",
       "Ahmed El-Sharkasy    8\n",
       "Cory House           8\n",
       "MVD NO               8\n",
       "Name: authors, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authors.value_counts().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "War Is Boring        305\n",
       "Caitlin Johnstone    227\n",
       "Jon Westenberg ðŸŒˆ     211\n",
       "Ethan Siegel         176\n",
       "Larry Kim            166\n",
       "Name: authors, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authors.authors.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 31331), (34645, 31331))"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LE = LabelEncoder()\n",
    "# LE.fit(authors.values.reshape(1,-1)[0])\n",
    "# X_train_author_LE = LE.transform(train_author_.values.reshape(1,-1)[0])\n",
    "# X_test_author_LE  = LE.transform(test_author_.values.reshape(1,-1)[0])\n",
    "\n",
    "# OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "# OHE.fit(X_train_author_LE.reshape(-1,1))\n",
    "\n",
    "# X_train_author_sparse = OHE.transform(X_train_author_LE.reshape(-1,1))\n",
    "# X_test_author_sparse  = OHE.transform(X_test_author_LE.reshape(-1,1))\n",
    "\n",
    "# X_train_author_sparse.shape, X_test_author_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** domain **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 2), (34645, 2))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "train_domain_ = pd.read_csv(os.path.join(PATH_TO_DATA,'train_domain.txt'),header=None,usecols=[0],names=['domains'])\n",
    "test_domain_ = pd.read_csv(os.path.join(PATH_TO_DATA,'test_domain.txt'),header=None,usecols=[0],names = ['domains'])\n",
    "\n",
    "domains = pd.concat([train_domain_,test_domain_],axis=0,sort=False)\n",
    "\n",
    "def toptwodomain(x):\n",
    "    if x=='medium.com': return 0\n",
    "    elif x=='hackernoon.com': return 1\n",
    "    else: return 2\n",
    "domains = domains.domains.map(toptwodomain)\n",
    "\n",
    "domains = pd.get_dummies(domains,drop_first=True)\n",
    "\n",
    "X_train_domain_sparse = domains.iloc[:train_domain_.shape[0],:].values\n",
    "X_test_domain_sparse  = domains.iloc[train_domain_.shape[0]:,:].values\n",
    "\n",
    "X_train_domain_sparse.shape, X_test_domain_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium.com              91601\n",
       "hackernoon.com           4367\n",
       "jw-webmagazine.com        147\n",
       "blog.medium.com            73\n",
       "thecoffeelicious.com       48\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains.iloc[:,0].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium.com              59522\n",
       "hackernoon.com           1938\n",
       "jw-webmagazine.com        143\n",
       "blog.medium.com            67\n",
       "thecoffeelicious.com       48\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_domain_.iloc[:,0].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medium.com                  32079\n",
       "hackernoon.com               2429\n",
       "towardsdatascience.com         17\n",
       "blog.usejournal.com             9\n",
       "journal.thriveglobal.com        7\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_domain_.iloc[:,0].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Date Columns **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_time = pd.read_csv(os.path.join(PATH_TO_DATA,'train_published.txt'),header=None,names=['date'],\n",
    "                                            parse_dates =['date'])\n",
    "X_test_time = pd.read_csv(os.path.join(PATH_TO_DATA,'test_published.txt'),header=None,names=['date'],\n",
    "                                         parse_dates =['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 1), (34645, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time.shape,X_test_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    new_df = pd.DataFrame(index=df.index)\n",
    "    hour = df['date'].apply(lambda ts: ts.hour)\n",
    "    new_df['morning'] = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    new_df[\"day\"] = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    new_df[\"evening\"] = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    new_df[\"night\"] = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    new_df['is_weekend'] = df['date'].dt.weekday.isin([6,7]).astype('int')\n",
    "#     new_df['year'] = df['date'].dt.year\n",
    "#     new_df['month'] = df['date'].dt.month\n",
    "#     new_df['weekday'] = df['date'].dt.weekday\n",
    "#     new_df['hour'] = df['date'].dt.hour    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 5), (34645, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time_features_sparse = add_time_features(X_train_time)\n",
    "X_test_time_features_sparse = add_time_features(X_test_time)\n",
    "X_train_time_features_sparse.shape,X_test_time_features_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morning</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   morning  day  evening  night  is_weekend\n",
       "0        0    0        0      1           0\n",
       "1        0    0        0      1           0\n",
       "2        0    0        0      1           1\n",
       "3        0    0        1      0           0\n",
       "4        0    1        0      0           0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time_features_sparse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** length features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA,'train_content.txt'),encoding=\"utf8\") as train_content:\n",
    "    length =[] \n",
    "    for line in train_content:\n",
    "        length.append(len(line))\n",
    "X_train_time_features_sparse['length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_content.txt'),encoding=\"utf8\") as test_content:\n",
    "    length =[] \n",
    "    for line in test_content:\n",
    "        length.append(len(line))\n",
    "X_test_time_features_sparse['length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'train_tags.txt'),encoding=\"utf8\") as train_tags:\n",
    "    length =[] \n",
    "    for line in train_tags:\n",
    "        length.append(len(line))\n",
    "X_train_time_features_sparse['tag_length'] = length\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA,'test_tags.txt'),encoding=\"utf8\") as test_tags:\n",
    "    length =[] \n",
    "    for line in test_tags:\n",
    "        length.append(len(line))\n",
    "X_test_time_features_sparse['tag_length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_time_features_sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_time_features_sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train_time_features_sparse)\n",
    "X_train_time_features_sparse[X_train_time_features_sparse.columns] = minmax.transform(X_train_time_features_sparse)\n",
    "X_test_time_features_sparse[X_test_time_features_sparse.columns] = minmax.transform(X_test_time_features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2743b329198>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(X_test_time_features_sparse.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morning</th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>length</th>\n",
       "      <th>tag_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.457944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005394</td>\n",
       "      <td>0.102804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026190</td>\n",
       "      <td>0.420561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029159</td>\n",
       "      <td>0.140187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   morning  day  evening  night  is_weekend    length  tag_length\n",
       "0      0.0  0.0      0.0    1.0         0.0  0.012452    0.457944\n",
       "1      0.0  0.0      0.0    1.0         0.0  0.012102    0.000000\n",
       "2      0.0  0.0      0.0    1.0         1.0  0.005394    0.102804\n",
       "3      0.0  0.0      1.0    0.0         0.0  0.026190    0.420561\n",
       "4      0.0  1.0      0.0    0.0         0.0  0.029159    0.140187"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_time_features_sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# minmax = MinMaxScaler()\n",
    "# minmax.fit(X_train_time_features_sparse)\n",
    "# X_train_time_features_sparse_norm =minmax.transform(X_train_time_features_sparse)\n",
    "# X_test_time_features_sparse_norm =minmax.transform(X_test_time_features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# minmax = StandardScaler()\n",
    "# minmax.fit(X_train_time_features_sparse)\n",
    "# X_train_time_features_sparse_norm =minmax.transform(X_train_time_features_sparse)\n",
    "# X_test_time_features_sparse_norm =minmax.transform(X_test_time_features_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "                         X_train_author_sparse, \n",
    "                         X_train_time_features_sparse]).tocsr()\n",
    "\n",
    "X_test_sparse = hstack([X_test_content_sparse, X_test_title_sparse,\n",
    "                        X_test_author_sparse, \n",
    "                        X_test_time_features_sparse]).tocsr()\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import sparse\n",
    "\n",
    "# # sparse.save_npz(\"./data/medium/X_train_sparse.npz\", X_train_sparse)\n",
    "# X_train_sparse = sparse.load_npz(\"./data/medium/X_train_sparse.npz\")\n",
    "\n",
    "# # sparse.save_npz(\"./data/medium/X_test_sparse.npz\", X_test_sparse)\n",
    "# X_test_sparse = sparse.load_npz(\"./data/medium/X_test_sparse.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5492824304336308"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lridge.fit(X_train_sparse,y_train)\n",
    "prediction = lridge.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_adjusted = prediction+ (4.33328 -prediction.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333280000000001"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_adjusted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_3.csv')#LB 1.82842 #CV 1.5492824304336308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_4.csv')\n",
    "#LB 1.79066 #CV 1.4881453048830608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "X_train_content_sparse = sparse.load_npz(\"./data/medium/X_train_content_sparse_total_bigram.npz\")\n",
    "X_test_content_sparse = sparse.load_npz(\"./data/medium/X_test_content_sparse_total_bigram.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 2), (62313, 352), (62313, 11))"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_domain_sparse.shape,X_train_author_sparse.shape,X_train_time_features_sparse_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = np.concatenate((X_train_author_sparse,X_train_time_features_sparse),axis=1)\n",
    "X_test_sparse = np.concatenate((X_test_author_sparse,X_test_time_features_sparse),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 5848), (34645, 5848))"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse1.shape,X_test_sparse1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 105850), (34645, 105850))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse = hstack([\n",
    "                          X_train_domain_sparse,\n",
    "                          X_train_content_sparse,\n",
    "#                           X_train_title_sparse,                          \n",
    "#                           X_train_tags_sparse,\n",
    "                          X_train_author_sparse,\n",
    "                          X_train_time_features_sparse]).tocsr()\n",
    "\n",
    "X_test_sparse = hstack([\n",
    "                         X_test_domain_sparse,\n",
    "                         X_test_content_sparse,\n",
    "#                          X_test_title_sparse,\n",
    "#                          X_test_tags_sparse,\n",
    "                         X_test_author_sparse,\n",
    "                         X_test_time_features_sparse]).tocsr()\n",
    "\n",
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')\n",
    "y_train = train_target['log_recommends'].values\n",
    "\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4878659037253157"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=500)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse)) # author+content+time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4878659037253157"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=500)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5626494038236158"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.563165334511272"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "lridge.fit(X_train_part_sparse,y_train_part)\n",
    "mean_absolute_error(y_valid,lridge.predict(X_valid_sparse)) ## added domain no titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34645, 105848)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.33328, 3.025346584063666)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge.fit(X_train_sparse,y_train)\n",
    "prediction = lridge.predict(X_test_sparse)\n",
    "\n",
    "pred_adjusted = prediction+ (4.33328 -prediction.mean())\n",
    "pred_adjusted.mean(),prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_5.csv')\n",
    "#LB 1.87163 #CV 1.6676447868344884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_6.csv')\n",
    "#LB 1.79381 #CV 1.5931624978304328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(prediction, './assignment6_submissions/assignment6_medium_submission_notadjusted_7.csv')\n",
    "#LB 2.09594 #CV 1.5931624978304328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_8.csv')\n",
    "#LB 1.79082 #cv 1.5864854295562016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_9.csv')\n",
    "#LB 1.79061  #cv sumbission with content and some features no title #cv 0.7  1.487730649228865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_10.csv')\n",
    "#LB  1.79047   #cv 0.7  1.487857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_11.csv')\n",
    "#LB  1.79120  #cv0.7  1.4878659037253157 # all common authors content time features.(test file correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(pred_adjusted, './assignment6_submissions/assignment6_medium_submission_adjusted_12.csv')\n",
    "#LB 1.79315   #cv0.7  1.4878659037253157 # all common authors content time features.(test file correctnow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=5),\n",
       "       error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=17, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'alpha': array([  1000.     ,   1668.10054,   2782.5594 ,   4641.58883,\n",
       "         7742.63683,  12915.49665,  21544.3469 ,  35938.13664,\n",
       "        59948.42503, 100000.     ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'alpha':np.logspace(3,5,10)}\n",
    "gs = GridSearchCV(lridge,param_grid=param_grid,n_jobs=-1,scoring='neg_mean_absolute_error',cv=TimeSeriesSplit(5))\n",
    "gs.fit(X_train_sparse1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 100000.0}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts = TimeSeriesSplit(5)\n",
    "alphas = np.logspace(1,3,5)\n",
    "lridgeCV = RidgeCV(alphas=alphas, scoring ='neg_mean_absolute_error',cv=ts)\n",
    "lridgeCV.fit(X_train_sparse1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000.0, array([ 0.00268131,  0.00402473,  0.00451589, ..., -0.02041138,\n",
       "         0.00301198, -0.0824426 ]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridgeCV.alpha_,lridgeCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    4.3s finished\n",
      "\n",
      "[2018-11-16 00:17:28] Features: 10/3 -- score: -1.6136425596076482[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.1s finished\n",
      "\n",
      "[2018-11-16 00:17:33] Features: 9/3 -- score: -1.612995721194757[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    3.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    3.8s finished\n",
      "\n",
      "[2018-11-16 00:17:37] Features: 8/3 -- score: -1.6127004388806323[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed:    3.6s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    3.8s finished\n",
      "\n",
      "[2018-11-16 00:17:42] Features: 7/3 -- score: -1.6126614938257287[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    3.6s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    3.7s finished\n",
      "\n",
      "[2018-11-16 00:17:46] Features: 6/3 -- score: -1.6126370872179436[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.8s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    3.4s finished\n",
      "\n",
      "[2018-11-16 00:17:50] Features: 5/3 -- score: -1.6126370247023256[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.2s finished\n",
      "\n",
      "[2018-11-16 00:17:53] Features: 4/3 -- score: -1.6126063072698678[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    3.2s finished\n",
      "\n",
      "[2018-11-16 00:17:57] Features: 3/3 -- score: -1.6125910066919549"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequentialFeatureSelector(clone_estimator=True, cv=5,\n",
       "             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=17, solver='auto', tol=0.001),\n",
       "             floating=False, forward=False, k_features=3, n_jobs=-1,\n",
       "             pre_dispatch='2*n_jobs', scoring='neg_mean_absolute_error',\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SequentialFeatureSelector(lridge, scoring='neg_mean_absolute_error', \n",
    "                                     verbose=2, k_features=3, forward=False, n_jobs=-1)\n",
    "\n",
    "selector.fit(X_train_time_features_sparse_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2', '8', '9')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70615878, -1.67787463, -1.60717713, -1.52979893, -1.49164347]),\n",
       " -1.602530587673937)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.73459287, -1.61540689, -1.49328139]), -1.6144270493039015)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=3,n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.6961397 , -1.66503504, -1.57940485, -1.51182441, -1.47267521]),\n",
       " -1.5850158420836533)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month feature + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70678507, -1.66486316, -1.574324  , -1.51921109, -1.47291004]),\n",
       " -1.5876186715697718)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=100)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # all common author, time  + removed hour,weekday, month,feature + title(5 ngram) + content on train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.70326353, -1.66437512, -1.5739535 , -1.51747089, -1.47336411]),\n",
       " -1.5864854295562016)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17,alpha=500)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() \n",
    "# only author common, time +content(train corpus) + removed hour,year, weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.79021982, -1.76944994, -1.67921221, -1.64240117, -1.58974062]),\n",
       " -1.6942047504117308)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +content(train corpus)+title(train corpus) + removed hour,weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75029904, -1.71864653, -1.63777537, -1.58549061, -1.53951762]),\n",
       " -1.6463458332448966)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +content(train corpus) + removed hour,weekday, month,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.80411863, -1.76962978, -1.66307081, -1.60140272, -1.54928635]),\n",
       " -1.6775016574455894)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time +title + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71383703, -1.67039702, -1.58319636, -1.52311857, -1.47406608]),\n",
       " -1.5929230112113395)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75029904, -1.71864653, -1.63777537, -1.58549061, -1.53951762]),\n",
       " -1.6463458332448966)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), time+ content(bigram, 50000) + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.7511022 , -1.71865133, -1.63760093, -1.58540646, -1.54022222]),\n",
       " -1.6465966285312508)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ content(bigram, 50000) + removed hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75552844, -1.72424925, -1.65143391, -1.59018058, -1.54537297]),\n",
       " -1.653353029870934)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ content(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.80465401, -1.76968117, -1.663078  , -1.60150013, -1.54924794]),\n",
       " -1.677632249227821)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time+ title(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71605728, -1.67320132, -1.58660663, -1.52797207, -1.47772642]),\n",
       " -1.5963127440676463)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(1000), domain, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.71481073, -1.67048868, -1.58324289, -1.52315781, -1.47411238]),\n",
       " -1.5931624978304328)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # only author(500), domain, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77988321, -1.75231114, -1.68516628, -1.63273228, -1.58802091]),\n",
       " -1.6876227639716952)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78005177, -1.75267851, -1.68626226, -1.6318807 , -1.58865579]),\n",
       " -1.6879058038621024)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78776372, -1.7541245 , -1.69207366, -1.62452704, -1.59107438]),\n",
       " -1.6899126623792564)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78788731, -1.75173271, -1.67609661, -1.60911423, -1.57100427]),\n",
       " -1.6791670269193824)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + tags - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.79597596, -1.76370831, -1.68977071, -1.62458945, -1.58708861]),\n",
       " -1.6922266082441344)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + small tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77895346, -1.74125258, -1.66932919, -1.60010456, -1.55931242]),\n",
       " -1.669790442385595)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.77880984, -1.74066763, -1.66815147, -1.6010553 , -1.5594876 ]),\n",
       " -1.669634365394295)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus + reduced domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78088846, -1.75069073, -1.65618224, -1.59819464, -1.55226787]),\n",
       " -1.6676447868344884)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus +reduced authorlist+ reduced domain + small ngram tags and titles - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78248074, -1.75609869, -1.68068572, -1.62029946, -1.59025814]),\n",
       " -1.685964551629732)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # same as baseline just with trained on full corpus +reduced authorlist+ reduced domain + \n",
    "#small ngram tags and titles(nostopword) - no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.78240041, -1.75600557, -1.68051004, -1.6203664 , -1.58921474]),\n",
       " -1.685699431471828)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#small ngram tags and titles(nostopword) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75596149, -1.72707715, -1.65358406, -1.59496739, -1.54873665]),\n",
       " -1.6560653469113027)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "# - no tags -no title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.7554402 , -1.72698805, -1.66269248, -1.60270939, -1.5585561 ]),\n",
       " -1.661277245275749)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#  - no tags -no title + more authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.75552844, -1.72424925, -1.65143391, -1.59018058, -1.54537297]),\n",
       " -1.653353029870934)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # content with bigrams +reduced authorlist+ reduced domain + \n",
    "#  - no tags -no title + less authors(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.74941744, -1.71131357, -1.63078541, -1.55365962, -1.51169151]),\n",
       " -1.6313735095426722)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lridge = Ridge(random_state=17)\n",
    "scores = cross_val_score(lridge,X_train_sparse1,y_train,cv=TimeSeriesSplit(5),n_jobs=-1,scoring='neg_mean_absolute_error')\n",
    "scores,scores.mean() # no content +reduced authorlist+ reduced domain + \n",
    "#  -no title + less authors(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the assignment. Much more credits will be given to the winners in this competition, check [course roadmap](https://mlcourse.ai/roadmap). Do not spoil the assignment and the competition - don't share high-performing kernels (with MAE < 1.5).\n",
    "\n",
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "<img src='../../img/kaggle_shakeup.png' width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
